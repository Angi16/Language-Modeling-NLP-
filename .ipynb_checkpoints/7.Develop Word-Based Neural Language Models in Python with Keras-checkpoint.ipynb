{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Develop Word-Based Neural Language Models in Python with Keras</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Language modeling involves predicting the next word in a sequence given the sequence of words already present.\n",
    "\n",
    ">A language model is a key element in many natural language processing models such as machine translation and speech recognition. The choice of how the language model is framed must match how the language model is intended to be used.\n",
    "\n",
    ">In this tutorial, ew will discover how the framing of a language model affects the skill of the model when generating short sequences from a nursery rhyme.\n",
    "\n",
    ">After completing this tutorial, we will know:\n",
    "\n",
    "*    The challenge of developing a good framing of a word-based language model for a given application.\n",
    "*    How to develop one-word, two-word, and line-based framings for word-based language models.\n",
    "*    How to generate sequences using a fit language model.\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "## <font color='blue'>Tutorial Overview</font>\n",
    "\n",
    "\n",
    "*    1.Framing Language Modeling\n",
    "*    2.Jack and Jill Nursery Rhyme\n",
    "*    3.Model 1: One-Word-In, One-Word-Out Sequences\n",
    "*    4.Model 2: Line-by-Line Sequence\n",
    "*    5.Model 3: Two-Words-In, One-Word-Out Sequence\n",
    "\n",
    "____________________________\n",
    "_______________________________\n",
    "\n",
    "## <font color='blue'>Framing Language Modeling</font>\n",
    "\n",
    "A statistical language model is learned from raw text and predicts the probability of the next word in the sequence given the words already present in the sequence.\n",
    "\n",
    "Language models are a key component in larger models for challenging natural language processing problems, like machine translation and speech recognition. They can also be developed as standalone models and used for generating new sequences that have the same statistical properties as the source text.\n",
    "\n",
    "Language models both learn and predict one word at a time. The training of the network involves providing sequences of words as input that are processed one at a time where a prediction can be made and learned for each input sequence.\n",
    "\n",
    "Similarly, when making predictions, the process can be seeded with one or a few words, then predicted words can be gathered and presented as input on subsequent predictions in order to build up a generated output sequence\n",
    "\n",
    "Therefore, each model will involve splitting the source text into input and output sequences, such that the model can learn to predict words.\n",
    "\n",
    "There are many ways to frame the sequences from a source text for language modeling.\n",
    "\n",
    "In this tutorial, we will explore 3 different ways of developing word-based language models in the Keras deep learning library.\n",
    "\n",
    "There is no single best approach, just different framings that may suit different applications.\n",
    "\n",
    "_________________________________\n",
    "__________________________________\n",
    "\n",
    "### <font color='blue'>Jack and Jill Nursery Rhyme</font>\n",
    "\n",
    "Jack and Jill is a simple nursery rhyme.\n",
    "\n",
    "It is comprised of 4 lines, as follows:\n",
    "\n",
    "    Jack and Jill went up the hill\n",
    "    To fetch a pail of water\n",
    "    Jack fell down and broke his crown\n",
    "    And Jill came tumbling after\n",
    "\n",
    "We will use this as our source text for exploring different framings of a word-based language model.\n",
    "\n",
    "We can define this text in Python as follows:\n",
    "\n",
    "```Python\n",
    " # source text\n",
    "data = \"\"\" Jack and Jill went up the hill\\n\n",
    "\t\tTo fetch a pail of water\\n\n",
    "\t\tJack fell down and broke his crown\\n\n",
    "\t\tAnd Jill came tumbling after\\n \"\"\"\n",
    "```\n",
    "______________________________________\n",
    "____________________________________\n",
    "\n",
    "## <font color='blue'>Model 1: One-Word-In, One-Word-Out Sequences</font>\n",
    "\n",
    "We can start with a very simple model.\n",
    "\n",
    "Given one word as input, the model will learn to predict the next word in the sequence.\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "```Python\n",
    "X,\t\ty\n",
    "Jack, \tand\n",
    "and,\tJill\n",
    "Jill,\twent\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to encode the text as integers.\n",
    "\n",
    "Each lowercase word in the source text is assigned a unique integer and we can convert the sequences of words to sequences of integers.\n",
    "\n",
    "Keras provides the Tokenizer class that can be used to perform this encoding. First, the Tokenizer is fit on the source text to develop the mapping from words to unique integers. Then sequences of text can be converted to sequences of integers by calling the texts_to_sequences() function.\n",
    "\n",
    "\n",
    "```Python\n",
    "# integer encode text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "```\n",
    "\n",
    "\n",
    "We will need to know the size of the vocabulary later for both defining the word embedding layer in the model, and for encoding output words using a one hot encoding.\n",
    "\n",
    "The size of the vocabulary can be retrieved from the trained Tokenizer by accessing the word_index attribute.\n",
    "\n",
    "```Python\n",
    "# determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "```\n",
    "\n",
    "Running this example, we can see that the size of the vocabulary is 21 words.\n",
    "\n",
    "We add one, because we will need to specify the integer for the largest encoded word as an array index, e.g. words encoded 1 to 21 with array indicies 0 to 21 or 22 positions.\n",
    "\n",
    "Next, we need to create sequences of words to fit the model with one word as input and one word as output.\n",
    "\n",
    "```Python\n",
    "# create word -> word sequences\n",
    "sequences = list()\n",
    "for i in range(1, len(encoded)):\n",
    "\tsequence = encoded[i-1:i+1]\n",
    "\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "```\n",
    "\n",
    "\n",
    "Running this piece shows that we have a total of 24 input-output pairs to train the network.\n",
    "Total Sequences: 24\n",
    "1\n",
    "\t\n",
    "Total Sequences: 24\n",
    "\n",
    "We can then split the sequences into input (X) and output elements (y). This is straightforward as we only have two columns in the data.\n",
    "\n",
    "```Python\n",
    "# split into X and y elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,0],sequences[:,1]\n",
    "```\n",
    "\n",
    "We will fit our model to predict a probability distribution across all words in the vocabulary. That means that we need to turn the output element from a single integer into a one hot encoding with a 0 for every word in the vocabulary and a 1 for the actual word that the value. This gives the network a ground truth to aim for from which we can calculate error and update the model.\n",
    "\n",
    "Keras provides the to_categorical() function that we can use to convert the integer to a one hot encoding while specifying the number of classes as the vocabulary size.\n",
    "\n",
    "```Python\n",
    "# one hot encode outputs\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "```\n",
    "\n",
    "We are now ready to define the neural network model.\n",
    "\n",
    "The model uses a learned word embedding in the input layer. This has one real-valued vector for each word in the vocabulary, where each word vector has a specified length. In this case we will use a 10-dimensional projection. The input sequence contains a single word, therefore the input_length=1.\n",
    "\n",
    "The model has a single hidden LSTM layer with 50 units. This is far more than is needed. The output layer is comprised of one neuron for each word in the vocabulary and uses a softmax activation function to ensure the output is normalized to look like a probability.\n",
    "\n",
    "```Python\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "The structure of the network can be summarized as follows:\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #\n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 1, 10)             220\n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 50)                12200\n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 22)                1122\n",
    "=================================================================\n",
    "Total params: 13,542\n",
    "Trainable params: 13,542\n",
    "Non-trainable params: 0\n",
    "\n",
    "_________________________________________________________________\n",
    "\n",
    "We will use this same general network structure for each example in this tutorial, with minor changes to the learned embedding layer.\n",
    "\n",
    "Next, we can compile and fit the network on the encoded text data. Technically, we are modeling a multi-class classification problem (predict the word in the vocabulary), therefore using the categorical cross entropy loss function. We use the efficient Adam implementation of gradient descent and track accuracy at the end of each epoch. The model is fit for 500 training epochs, again, perhaps more than is needed.\n",
    "\n",
    "The network configuration was not tuned for this and later experiments; an over-prescribed configuration was chosen to ensure that we could focus on the framing of the language model.\n",
    "```Python\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=500, verbose=2)\n",
    "```\n",
    "\n",
    "\n",
    "After the model is fit, we test it by passing it a given word from the vocabulary and having the model predict the next word. Here we pass in ‘Jack‘ by encoding it and calling model.predict_classes() to get the integer output for the predicted word. This is then looked up in the vocabulary mapping to give the associated word.\n",
    "\n",
    "```Python\n",
    "# evaluate\n",
    "in_text = 'Jack'\n",
    "print(in_text)\n",
    "encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "encoded = array(encoded)\n",
    "yhat = model.predict_classes(encoded, verbose=0)\n",
    "for word, index in tokenizer.word_index.items():\n",
    "\tif index == yhat:\n",
    "\t\tprint(word)\n",
    "\n",
    "This process could then be repeated a few times to build up a generated sequence of words.\n",
    "\n",
    "To make this easier, we wrap up the behavior in a function that we can call by passing in our model and the seed word.\n",
    "```Python\n",
    "# generate a sequence from the model\n",
    "def generate_seq(model, tokenizer, seed_text, n_words):\n",
    "\tin_text, result = seed_text, seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\tencoded = array(encoded)\n",
    "\t\t# predict a word in the vocabulary\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text, result = out_word, result + ' ' + out_word\n",
    "\treturn result\n",
    "```\n",
    "\n",
    "We can time all of this together. The complete code listing is provided below.\n",
    "\n",
    "```Python\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# generate a sequence from the model\n",
    "def generate_seq(model, tokenizer, seed_text, n_words):\n",
    "\tin_text, result = seed_text, seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\tencoded = array(encoded)\n",
    "\t\t# predict a word in the vocabulary\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text, result = out_word, result + ' ' + out_word\n",
    "\treturn result\n",
    "\n",
    "# source text\n",
    "data = \"\"\" Jack and Jill went up the hill\\n\n",
    "\t\tTo fetch a pail of water\\n\n",
    "\t\tJack fell down and broke his crown\\n\n",
    "\t\tAnd Jill came tumbling after\\n \"\"\"\n",
    "# integer encode text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "# determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# create word -> word sequences\n",
    "sequences = list()\n",
    "for i in range(1, len(encoded)):\n",
    "\tsequence = encoded[i-1:i+1]\n",
    "\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "# split into X and y elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,0],sequences[:,1]\n",
    "# one hot encode outputs\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=500, verbose=2)\n",
    "# evaluate\n",
    "print(generate_seq(model, tokenizer, 'Jack', 6))\n",
    "```\n",
    "\n",
    "Running the example prints the loss and accuracy each training epoch.\n",
    "```\n",
    "...\n",
    "Epoch 496/500\n",
    "0s - loss: 0.2358 - acc: 0.8750\n",
    "Epoch 497/500\n",
    "0s - loss: 0.2355 - acc: 0.8750\n",
    "Epoch 498/500\n",
    "0s - loss: 0.2352 - acc: 0.8750\n",
    "Epoch 499/500\n",
    "0s - loss: 0.2349 - acc: 0.8750\n",
    "Epoch 500/500\n",
    "0s - loss: 0.2346 - acc: 0.8750\n",
    "```\n",
    "\n",
    "We can see that the model does not memorize the source sequences, likely because there is some ambiguity in the input sequences, for example:\n",
    "```\n",
    "jack => and\n",
    "jack => fell\n",
    "```\n",
    "\n",
    "And so on.\n",
    "\n",
    "At the end of the run, ‘Jack‘ is passed in and a prediction or new sequence is generated.\n",
    "\n",
    "We get a reasonable sequence as output that has some elements of the source.\n",
    "Jack and jill came tumbling after down\n",
    "1\n",
    "\t\n",
    "```\n",
    "Jack and jill came tumbling after down\n",
    "```\n",
    "This is a good first cut language model, but does not take full advantage of the LSTM’s ability to handle sequences of input and disambiguate some of the ambiguous pairwise sequences by using a broader context.\n",
    "\n",
    "## <font color='blue'>Model 2: Line-by-Line Sequence</font>\n",
    "\n",
    "Another approach is to split up the source text line-by-line, then break each line down into a series of words that build up.\n",
    "\n",
    "For example:\n",
    "```\n",
    "X,\t\t\t\t\t\t\t\t\ty\n",
    "_, _, _, _, _, Jack, \t\t\t\tand\n",
    "_, _, _, _, Jack, and \t\t\t\tJill\n",
    "_, _, _, Jack, and, Jill,\t\t\twent\n",
    "_, _, Jack, and, Jill, went,\t\tup\n",
    "_, Jack, and, Jill, went, up,\t\tthe\n",
    "Jack, and, Jill, went, up, the,\t\thill\n",
    "```\n",
    "\n",
    "This approach may allow the model to use the context of each line to help the model in those cases where a simple one-word-in-and-out model creates ambiguity.\n",
    "\n",
    "In this case, this comes at the cost of predicting words across lines, which might be fine for now if we are only interested in modeling and generating lines of text.\n",
    "\n",
    "Note that in this representation, we will require a padding of sequences to ensure they meet a fixed length input. This is a requirement when using Keras.\n",
    "\n",
    "First, we can create the sequences of integers, line-by-line by using the Tokenizer already fit on the source text.\n",
    "```Python\n",
    "# create line-based sequences\n",
    "sequences = list()\n",
    "for line in data.split('\\n'):\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(encoded)):\n",
    "\t\tsequence = encoded[:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "```\n",
    "\n",
    "Next, we can pad the prepared sequences. We can do this using the pad_sequences() function provided in Keras. This first involves finding the longest sequence, then using that as the length by which to pad-out all other sequences.\n",
    "```Python\n",
    "# pad input sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "```\n",
    "Next, we can split the sequences into input and output elements, much like before.\n",
    "```Python\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "```\n",
    "\n",
    "The model can then be defined as before, except the input sequences are now longer than a single word. Specifically, they are max_length-1 in length, -1 because when we calculated the maximum length of sequences, they included the input and output elements.\n",
    "```Python\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=500, verbose=2)\n",
    "```\n",
    "\n",
    "We can use the model to generate new sequences as before. The generate_seq() function can be updated to build up an input sequence by adding predictions to the list of input words each iteration.\n",
    "```Python\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pre-pad sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\treturn in_text\n",
    "```\n",
    "\n",
    "Tying all of this together, the complete code example is provided below.\n",
    "```Python\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pre-pad sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\treturn in_text\n",
    "\n",
    "# source text\n",
    "data = \"\"\" Jack and Jill went up the hill\\n\n",
    "\t\tTo fetch a pail of water\\n\n",
    "\t\tJack fell down and broke his crown\\n\n",
    "\t\tAnd Jill came tumbling after\\n \"\"\"\n",
    "# prepare the tokenizer on the source text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "# determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# create line-based sequences\n",
    "sequences = list()\n",
    "for line in data.split('\\n'):\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(encoded)):\n",
    "\t\tsequence = encoded[:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "# pad input sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=500, verbose=2)\n",
    "# evaluate model\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'Jack', 4))\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'Jill', 4))\n",
    "```\n",
    "\n",
    "Running the example achieves a better fit on the source data. The added context has allowed the model to disambiguate some of the examples.\n",
    "\n",
    "There are still two lines of text that start with ‘Jack‘ that may still be a problem for the network.\n",
    "```\n",
    "...\n",
    "Epoch 496/500\n",
    "0s - loss: 0.1039 - acc: 0.9524\n",
    "Epoch 497/500\n",
    "0s - loss: 0.1037 - acc: 0.9524\n",
    "Epoch 498/500\n",
    "0s - loss: 0.1035 - acc: 0.9524\n",
    "Epoch 499/500\n",
    "0s - loss: 0.1033 - acc: 0.9524\n",
    "Epoch 500/500\n",
    "0s - loss: 0.1032 - acc: 0.9524\n",
    "```\n",
    "\n",
    "At the end of the run, we generate two sequences with different seed words: ‘Jack‘ and ‘Jill‘.\n",
    "\n",
    "The first generated line looks good, directly matching the source text. The second is a bit strange. This makes sense, because the network only ever saw ‘Jill‘ within an input sequence, not at the beginning of the sequence, so it has forced an output to use the word ‘Jill‘, i.e. the last line of the rhyme.\n",
    "```\n",
    "Jack fell down and broke\n",
    "Jill jill came tumbling after\n",
    "\n",
    "```\n",
    "\n",
    "This was a good example of how the framing may result in better new lines, but not good partial lines of input.\n",
    "\n",
    "## <font color='blue'>Model 3: Two-Words-In, One-Word-Out Sequence</font>\n",
    "\n",
    "We can use an intermediate between the one-word-in and the whole-sentence-in approaches and pass in a sub-sequences of words as input.\n",
    "\n",
    "This will provide a trade-off between the two framings allowing new lines to be generated and for generation to be picked up mid line.\n",
    "\n",
    "We will use 3 words as input to predict one word as output. The preparation of the sequences is much like the first example, except with different offsets in the source sequence arrays, as follows:\n",
    "```Python\n",
    "# encode 2 words -> 1 word\n",
    "sequences = list()\n",
    "for i in range(2, len(encoded)):\n",
    "\tsequence = encoded[i-2:i+1]\n",
    "\tsequences.append(sequence)\n",
    "```\n",
    "\n",
    "The complete example is listed below\n",
    "```Python\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pre-pad sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\treturn in_text\n",
    "\n",
    "# source text\n",
    "data = \"\"\" Jack and Jill went up the hill\\n\n",
    "\t\tTo fetch a pail of water\\n\n",
    "\t\tJack fell down and broke his crown\\n\n",
    "\t\tAnd Jill came tumbling after\\n \"\"\"\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "# retrieve vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# encode 2 words -> 1 word\n",
    "sequences = list()\n",
    "for i in range(2, len(encoded)):\n",
    "\tsequence = encoded[i-2:i+1]\n",
    "\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "# pad sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=500, verbose=2)\n",
    "# evaluate model\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'Jack and', 5))\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'And Jill', 3))\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'fell down', 5))\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'pail of', 5))\n",
    "```\n",
    "\n",
    "Running the example again gets a good fit on the source text at around 95% accuracy.\n",
    "```\n",
    "...\n",
    "Epoch 496/500\n",
    "0s - loss: 0.0685 - acc: 0.9565\n",
    "Epoch 497/500\n",
    "0s - loss: 0.0685 - acc: 0.9565\n",
    "Epoch 498/500\n",
    "0s - loss: 0.0684 - acc: 0.9565\n",
    "Epoch 499/500\n",
    "0s - loss: 0.0684 - acc: 0.9565\n",
    "Epoch 500/500\n",
    "0s - loss: 0.0684 - acc: 0.9565\n",
    "```\n",
    "\n",
    "We look at 4 generation examples, two start of line cases and two starting mid line.\n",
    "\n",
    "*Jack and jill went up the hill*\n",
    "*And Jill went up the*\n",
    "*fell down and broke his crown and*\n",
    "*pail of water jack fell down and*\n",
    "\n",
    "\n",
    "The first start of line case generated correctly, but the second did not. The second case was an example from the 4th line, which is ambiguous with content from the first line. Perhaps a further expansion to 3 input words would be better.\n",
    "\n",
    "The two mid-line generation examples were generated correctly, matching the source text.\n",
    "\n",
    "We can see that the choice of how the language model is framed and the requirements on how the model will be used must be compatible. That careful design is required when using language models in general, perhaps followed-up by spot testing with sequence generation to confirm model requirements have been met.\n",
    "\n",
    "## <font color='blue'>Extensions</font>\n",
    "\n",
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    "*  **Whole Rhyme as Sequence.** Consider updating one of the above examples to build up the entire rhyme as an input sequence. The model should be able to generate the entire thing given the seed of the first word, demonstrate this.\n",
    "*    **Pre-Trained Embeddings.** Explore using pre-trained word vectors in the embedding instead of learning the embedding as part of the model. This would not be required on such a small source text, but could be good practice.\n",
    "*    **Character Models.** Explore the use of a character-based language model for the source text instead of the word-based approach demonstrated in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
